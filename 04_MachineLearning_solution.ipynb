{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning using scikit-learn\n",
    "\n",
    "There are two kinds of machine learning algorithms: *supervised* and *unsupervised* learning. \n",
    "\n",
    "Examples for supervised algorithms: classification, regression, etc.\n",
    "Examples for unsupervised algorithms: clustering, dimension reduction, etc.\n",
    "\n",
    "## scikit-learn estimators\n",
    "\n",
    "Scikit-learn strives to have a uniform interface across all objects. Given a scikit-learn *estimator* named `model`, the following methods are available:\n",
    "\n",
    "- Available in **all estimators**\n",
    "  + `model.fit()` : Fit training data. For supervised learning applications,\n",
    "    this accepts two arguments: the data `X` and the labels `y` (e.g., `model.fit(X, y)`).\n",
    "    For unsupervised learning applications, ``fit`` takes only a single argument,\n",
    "    the data `X` (e.g. `model.fit(X)`).\n",
    "    \n",
    "- Available in **supervised estimators**\n",
    "  + `model.predict()` : Given a trained model, predict the label of a new set of data.\n",
    "    This method accepts one argument, the new data `X_new` (e.g., `model.predict(X_new)`),\n",
    "    and returns the learned label for each object in the array.\n",
    "  + `model.fit_predict()`: Fits and predicts at the same time.  \n",
    "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
    "    this method, which returns the probability that a new observation has each categorical label.\n",
    "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "  + `model.score()` : An indication of how well the model fits the training data.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
    "  \n",
    "## Data in scikit-learn\n",
    "\n",
    "Data in scikit-learn, with very few exceptions, is assumed to be stored as a\n",
    "**two-dimensional array** of size `[n_samples, n_features]`. Many algorithms also accept ``scipy.sparse`` matrices of the same shape.\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g., classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner. Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "  \n",
    "### Numerical vs. categorical\n",
    "\n",
    "What if you have categorical features?  For example, imagine there is dataset containing the color of the\n",
    "iris:\n",
    "\n",
    "    color in [red, blue, purple]\n",
    "\n",
    "You might be tempted to assign numbers to these features, i.e. *red=1, blue=2, purple=3*\n",
    "but in general **this is a bad idea**.  Estimators tend to operate under the assumption that\n",
    "numerical features lie on some continuous scale, so, for example, 1 and 2 are more alike\n",
    "than 1 and 3, and this is often not the case for categorical features.\n",
    "\n",
    "A better strategy is to give each category its own dimension.  \n",
    "The enriched iris feature set would hence be in this case:\n",
    "\n",
    "- sepal length in cm\n",
    "- sepal width in cm\n",
    "- petal length in cm\n",
    "- petal width in cm\n",
    "- color=purple (1.0 or 0.0)\n",
    "- color=blue (1.0 or 0.0)\n",
    "- color=red (1.0 or 0.0)\n",
    "\n",
    "Note that using many of these categorical features may result in data which is better\n",
    "represented as a **sparse matrix**, as we'll see with the text classification example\n",
    "below.\n",
    "\n",
    "#### Using the DictVectorizer to encode categorical features\n",
    "\n",
    "When the source data has a list of dicts where the values are either string names of categories or numerical values, you can use the `DictVectorizer` class to compute the boolean expansion of the categorical features while leaving the numerical features unimpacted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measurements = [\n",
    "    {'city': 'Dubai', 'temperature': 33.},\n",
    "    {'city': 'London', 'temperature': 12.},\n",
    "    {'city': 'San Francisco', 'temperature': 18.},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "tf_measurements = vec.fit_transform(measurements)\n",
    "tf_measurements.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Clustering using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#disable some annoying warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the iris datasets\n",
    "import sklearn.datasets\n",
    "\n",
    "data = sklearn.datasets.load_iris()\n",
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "iris_pred = KMeans(n_clusters=3, random_state = 102).fit_predict(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "colors = sns.color_palette()\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.scatter(data.data[:, 0], data.data[:, 1], c=[colors[i] for i in iris_pred], s=40)\n",
    "plt.title('KMeans-3 clusterer')\n",
    "plt.xlabel(data.feature_names[0])\n",
    "plt.ylabel(data.feature_names[1])\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.scatter(data.data[:, 0], data.data[:, 1], c=[colors[i] for i in data.target],s=40)\n",
    "plt.title('Ground Truth')\n",
    "plt.xlabel(data.feature_names[0])\n",
    "plt.ylabel(data.feature_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised classification using decision trees\n",
    "\n",
    "Well, the result is not that great. Let's use a supervised classifier.\n",
    "\n",
    "First, split our data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.cross_validation\n",
    "\n",
    "data_train, data_test, target_train, target_test = sklearn.cross_validation.train_test_split(\n",
    "    data.data, data.target, test_size=0.20, random_state = 5)\n",
    "\n",
    "print(data.data.shape, data_train.shape, data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use a *DecisionTree* to learn a model and test our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "instance = DecisionTreeClassifier()\n",
    "r = instance.fit(data_train, target_train)\n",
    "target_predict = instance.predict(data_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Prediction accuracy: ', accuracy_score(target_predict, target_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction using MDS and PCA\n",
    "\n",
    "If we go back to our K-Means example, the clustering doesn't really make sense. However, we are just looking at two out of four dimensions. So, we can't really see the real distances/similarities between items. Dimension reduction techniques reduce the number of dimensions, while preserving the inner structure of the higher dimensions. We take a look at two of them: Multi Dimensional Scaling (MDS) and Principal Component Analysis (PCA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "#create mds instance\n",
    "mds = manifold.MDS(n_components=2, random_state=5)\n",
    "\n",
    "#fit the model and get the embedded coordinates\n",
    "pos = mds.fit(data.data).embedding_\n",
    "\n",
    "plt.scatter(pos[:, 0], pos[:, 1], s=20, c=[colors[i] for i in data.target])\n",
    "\n",
    "#create a legend since we just have one plot and not three fake the legend using patches\n",
    "import matplotlib.patches as mpatches\n",
    "patches = [ mpatches.Patch(color=colors[i], label=data.target_names[i]) for i in range(3) ]\n",
    "plt.legend(handles=patches)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compare with PCA\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca_pos = pca.fit(data.data).transform(data.data)\n",
    "\n",
    "mds_pos = mds.fit(data.data).embedding_\n",
    "\n",
    "plt.figure(figsize=[20,7])\n",
    "plt.subplot(121)\n",
    "plt.scatter(mds_pos[:, 0], mds_pos[:, 1], s=30, c=[colors[i] for i in data.target])\n",
    "plt.title('MDS')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(pca_pos[:, 0], pca_pos[:, 1], s=30, c=[colors[i] for i in data.target])\n",
    "plt.title('PCA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like versicolor and virginicia are more similar than setosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK\n",
    "\n",
    "> Create an interactive colored plot of the Iris dataset projected in 2D using MDS. The color should correspong to the result of a K-Means clusterin alrogithm where the user can interactivly define the number of clusters between 1 and 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interact\n",
    "colors = sns.color_palette(n_colors=10)\n",
    "\n",
    "\n",
    "mds_pos = mds.fit(data.data).embedding_\n",
    "\n",
    "@interact(n_clusters=(1,10))\n",
    "def draw_plot(n_clusters):\n",
    "    instance = KMeans(n_clusters=n_clusters, random_state = 102)\n",
    "    clusters_assignment = instance.fit_predict(data.data)\n",
    "    plt.scatter(mds_pos[:, 0], mds_pos[:, 1], s=20, c=[colors[i] for i in clusters_assignment])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thanks!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
